{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7G+nOVsgHaGVeva/SNDh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gr8318/CC/blob/main/DL_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeP3x37QOOhi"
      },
      "outputs": [],
      "source": [
        "#Implementing Logic Gates using Sigmoid Neuron\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        " return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def and_gate(x1, x2):\n",
        " w1, w2, b = 0.5, 0.5, -0.8\n",
        " z = w1 * x1 + w2 * x2 + b\n",
        " return 1 if sigmoid(z) > 0.5 else 0\n",
        "\n",
        "def not_gate(x):\n",
        "   w, b = -1, 0.5\n",
        "   z = w * x + b\n",
        "   return 1 if sigmoid(z) > 0.5 else 0\n",
        "\n",
        "def or_gate(x1, x2):\n",
        " w1, w2, b = 0.5, 0.5, -0.3\n",
        " z = w1 * x1 + w2 * x2 + b\n",
        " return 1 if sigmoid(z) > 0.5 else 0\n",
        "\n",
        "print(\"--- AND Gate ---\")\n",
        "print(f\"AND(0, 0) = {and_gate(0, 0)}\")\n",
        "print(f\"AND(0, 1) = {and_gate(0, 1)}\")\n",
        "print(f\"AND(1, 0) = {and_gate(1, 0)}\")\n",
        "print(f\"AND(1, 1) = {and_gate(1, 1)}\")\n",
        "print(\"-\" * 20)\n",
        "print(\"--- OR Gate ---\")\n",
        "print(f\"OR(0, 0) = {or_gate(0, 0)}\")\n",
        "print(f\"OR(0, 1) = {or_gate(0, 1)}\")\n",
        "print(f\"OR(1, 0) = {or_gate(1, 0)}\")\n",
        "print(f\"OR(1, 1) = {or_gate(1, 1)}\")\n",
        "print(\"-\" * 20)\n",
        "print(\"--- NOT Gate ---\")\n",
        "print(f\"NOT(0) = {not_gate(0)}\")\n",
        "print(f\"NOT(1) = {not_gate(1)}\")\n",
        "print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training a Neural Network with Backpropagation to Learn XOR Gate\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        " return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_derivative(x):\n",
        " return x * (1 - x)\n",
        "\n",
        "X = np.array([[0, 0],\n",
        " [0, 1],\n",
        " [1, 0],\n",
        " [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "input_layer_neurons = 2\n",
        "hidden_layer_neurons = 2\n",
        "output_neurons = 1\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "np.random.seed(42)\n",
        "wh = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
        "bh = np.random.uniform(size=(1, hidden_layer_neurons))\n",
        "wo = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
        "bo = np.random.uniform(size=(1, output_neurons))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        " hidden_input = np.dot(X, wh) + bh\n",
        " hidden_output = sigmoid(hidden_input)\n",
        " final_input = np.dot(hidden_output, wo) + bo\n",
        " predicted_output = sigmoid(final_input)\n",
        "\n",
        " error = y - predicted_output\n",
        " d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        " error_hidden_layer = d_predicted_output.dot(wo.T)\n",
        " d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_output)\n",
        "\n",
        " wo += hidden_output.T.dot(d_predicted_output) * learning_rate\n",
        " bo += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        " wh += X.T.dot(d_hidden_layer) * learning_rate\n",
        " bh += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "print(\"Final output after training:\")\n",
        "print(np.round(predicted_output, 3))"
      ],
      "metadata": {
        "id": "rlQuPNlMPWy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple Perceptron Model for AND Logic Gate\n",
        "import numpy as np\n",
        "class Perceptron:\n",
        " def __init__(self, learning_rate=0.01, n_iters=10):\n",
        "  self.lr = learning_rate\n",
        "  self.n_iters = n_iters\n",
        "  self.weights = None\n",
        "  self.bias = None\n",
        " def fit(self, x, y):\n",
        "  n_samples, n_features = x.shape\n",
        "  self.weights = np.zeros(n_features)\n",
        "  self.bias = 0\n",
        "  for _ in range(self.n_iters):\n",
        "   for idx, x_i in enumerate(x):\n",
        "    linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "    y_predicted = self.activation(linear_output)\n",
        "    update = self.lr * (y[idx] - y_predicted)\n",
        "    self.weights += update * x_i\n",
        "    self.bias += update\n",
        " def activation(self, x):\n",
        "  return 1 if x >= 0 else 0\n",
        " def predict(self, x):\n",
        "  linear_output = np.dot(x, self.weights) + self.bias\n",
        "  y_predicted = np.array([self.activation(val) for val in linear_output])\n",
        "  return y_predicted\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        " x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        " y = np.array([0, 0, 0, 1])\n",
        " p = Perceptron(learning_rate=0.1, n_iters=10)\n",
        " p.fit(x, y)\n",
        " predictions = p.predict(x)\n",
        " print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "BVijRi2bPjlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification of Iris Dataset using a Single Layer Perceptron (SLP) with TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        " X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = keras.Sequential([\n",
        " layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        " layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        " optimizer='adam',\n",
        " loss='sparse_categorical_crossentropy',\n",
        " metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        " X_train, y_train,\n",
        "  epochs=50,\n",
        " validation_data=(X_test, y_test),\n",
        " verbose=1\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "vNPneOS_QKdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple Perceptron Model for Binary Classification on Iris Dataset\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "class Perceptron:\n",
        " def __init__(self, learning_rate=0.01, n_iters=10):\n",
        "  self.lr = learning_rate\n",
        "  self.n_iters = n_iters\n",
        "  self.weights = None\n",
        "  self.bias = None\n",
        " def fit(self, X, y):\n",
        "  n_samples, n_features = X.shape\n",
        "  self.weights = np.zeros(n_features)\n",
        "  self.bias = 0\n",
        "  for _ in range(self.n_iters):\n",
        "   for idx, x_i in enumerate(X):\n",
        "    linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "    y_predicted = self.activation(linear_output)\n",
        "    update = self.lr * (y[idx] - y_predicted)\n",
        "    self.weights += update * x_i\n",
        "    self.bias += update\n",
        " def activation(self, x):\n",
        "  return 1 if x >= 0 else 0\n",
        " def predict(self, X):\n",
        "  if X.ndim == 1:\n",
        "   linear_output = [np.dot(X, self.weights) + self.bias]\n",
        "  else:\n",
        "   linear_output = np.dot(X, self.weights) + self.bias\n",
        "  y_predicted = np.array([self.activation(val) for val in linear_output])\n",
        "  return y_predicted\n",
        "if __name__ == \"__main__\":\n",
        " iris = load_iris()\n",
        " X = iris.data\n",
        " y = iris.target\n",
        " y = np.where(y == 0, 0, 1)\n",
        " p = Perceptron(learning_rate=0.1, n_iters=10)\n",
        " p.fit(X, y)\n",
        " predictions = p.predict(X)\n",
        " print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "id": "ZMSazGQfQk6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handwritten Digit Recognition on MNIST using a Multilayer Perceptron\n",
        "import tensorflow as tf\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "20\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "mlp = MLPClassifier(\n",
        " hidden_layer_sizes=(100,),\n",
        " max_iter=10,\n",
        " solver='adam',\n",
        " random_state=42\n",
        ")\n",
        "\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "21\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "axes = axes.ravel()\n",
        "for i in range(10):\n",
        " axes[i].imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        " axes[i].set_title(f'Pred: {y_pred[i]}')\n",
        " axes[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D5kHwQbWQ6n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting Temperature Over Time Using RBF Regression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "time = np.array([6, 9, 12, 15, 18, 21]).reshape(-1, 1)\n",
        "temperature = np.array([15, 20, 30, 32, 25, 18])\n",
        "\n",
        "def rbf(x, c, gamma=0.1):\n",
        " return np.exp(-gamma * (x - c)**2)\n",
        "\n",
        "centers = time.flatten()\n",
        "gamma = 0.1\n",
        "\n",
        "X_rbf = np.zeros((len(time), len(centers)))\n",
        "for i, c in enumerate(centers):\n",
        " X_rbf[:, i] = rbf(time.flatten(), c, gamma)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_rbf, temperature)\n",
        "\n",
        "time_test = np.linspace(0, 24, 100).reshape(-1, 1)\n",
        "X_rbf_test = np.zeros((len(time_test), len(centers)))\n",
        "for i, c in enumerate(centers):\n",
        " X_rbf_test[:, i] = rbf(time_test.flatten(), c, gamma)\n",
        "\n",
        "temp_pred = model.predict(X_rbf_test)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(time, temperature, color='red', label='Training Data')\n",
        "plt.plot(time_test, temp_pred, label='RBF Regression Prediction', color='blue')\n",
        "plt.title('Predicting Temperature Over Time Using RBF')\n",
        "plt.xlabel('Time of Day (Hours)')\n",
        "plt.ylabel('Temperature (Â°C)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KkT2UF4rRUK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solving XOR Problem Using a Neural Network (TensorFlow/Keras)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "X = np.array([[0, 0],\n",
        " [0, 1],\n",
        " [1, 0],\n",
        " [1, 1]], dtype=np.float32)\n",
        "y = np.array([0, 1, 1, 0], dtype=np.float32)\n",
        "\n",
        "model = Sequential([\n",
        " Dense(4, activation='relu', input_shape=(2,)),\n",
        " Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.1),\n",
        " loss='binary_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "\n",
        "print(\"Training progress:\")\n",
        "history = model.fit(X, y, epochs=1000, verbose=0)\n",
        "\n",
        "for i in range(0, 1000, 100):\n",
        " loss, acc = history.history['loss'][i], history.history['accuracy'][i]\n",
        " print(f\"Epoch {i}: loss = {loss:.4f}, accuracy = {acc:.4f}\")\n",
        "\n",
        "predictions = model.predict(X)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "print(\"\\nFinal Predictions:\")\n",
        "for i in range(len(X)):\n",
        " print((f\"Input: {X[i]}, Predicted: {predicted_classes[i][0]} \"\n",
        "  f\"(prob: {predictions[i][0]:.4f}), Actual: {y[i]}\"))\n",
        "\n",
        "print(\"\\nModel architecture:\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LPvsnDxoRpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing XOR Problem Using a Custom Neural Network (NumPy)\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        " def __init__(self, input_size, hidden_size, output_size):\n",
        "  np.random.seed(42)\n",
        "\n",
        "  self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
        "  self.b1 = np.zeros((1, hidden_size))\n",
        "  self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
        "  self.b2 = np.zeros((1, output_size))\n",
        "\n",
        " def sigmoid(self, z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        " def forward(self, X):\n",
        "  self.z1 = np.dot(X, self.W1) + self.b1\n",
        "  self.a1 = np.maximum(0, self.z1)\n",
        "  self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "  self.a2 = self.sigmoid(self.z2)\n",
        "  return self.a2\n",
        "\n",
        " def backward(self, X, y, lr=0.1):\n",
        "  m = X.shape[0]\n",
        "  dz2 = self.a2 - y.reshape(-1, 1)\n",
        "  dW2 = np.dot(self.a1.T, dz2) / m\n",
        "  db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "  dz1 = np.dot(dz2, self.W2.T) * (self.z1 > 0)\n",
        "  dW1 = np.dot(X.T, dz1) / m\n",
        "  db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "  self.W1 -= lr * dW1\n",
        "  self.b1 -= lr * db1\n",
        "  self.W2 -= lr * dW2\n",
        "  self.b2 -= lr * db2\n",
        "\n",
        " def train(self, X, y, epochs=1000, lr=0.1):\n",
        "  for epoch in range(epochs):\n",
        "   self.forward(X)\n",
        "   self.backward(X, y, lr)\n",
        "   if epoch % 100 == 0:\n",
        "    loss = -np.mean(y * np.log(self.a2 + 1e-8) + (1 - y) * np.log(1 - self.a2 + 1e-8))\n",
        "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        " def predict(self, X, threshold=0.5):\n",
        "  proba = self.forward(X)\n",
        "  return (proba >= threshold).astype(int)\n",
        "\n",
        "X = np.array([[0, 0],\n",
        " [0, 1],\n",
        " [1, 0],\n",
        " [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "nn.train(X, y, epochs=1000, lr=0.1)\n",
        "\n",
        "predictions = nn.predict(X)\n",
        "print(\"\\nFinal Predictions:\")\n",
        "for i in range(len(X)):\n",
        " print(f\"Input: {X[i]}, Predicted: {predictions[i][0]}, Actual: {y[i]}\")"
      ],
      "metadata": {
        "id": "7dqvoATPSN4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR-10 Classification with a Simple Fully Connected Neural Network (TensorFlow/Keras)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, models, layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "train_images = train_images.reshape((train_images.shape[0], -1)) / 255.0\n",
        "test_images = test_images.reshape((test_images.shape[0], -1)) / 255.0\n",
        "\n",
        "class_names = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer',\n",
        " 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "\n",
        "model = models.Sequential([\n",
        " layers.Dense(512, activation='relu', input_shape=(3072,)),\n",
        " layers.Dense(256, activation='relu'),\n",
        " layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        " optimizer='adam',\n",
        " loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        " metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        " train_images, train_labels,\n",
        " epochs=10,\n",
        " batch_size=64,\n",
        " validation_data=(test_images, test_labels),\n",
        " verbose=2\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc:.2f}\")\n",
        "\n",
        "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
        "predictions = probability_model.predict(test_images, verbose=0)\n",
        "print(f\"Predicted label: {class_names[np.argmax(predictions[0])]} | \"\n",
        " f\"True label: {class_names[test_labels[0][0]]}\")\n",
        "\n",
        "num_images_to_show = 5\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(num_images_to_show):\n",
        " plt.subplot(1, num_images_to_show, i + 1)\n",
        " plt.xticks([])\n",
        " plt.yticks([])\n",
        " plt.grid(False)\n",
        "\n",
        " plt.imshow(test_images[i].reshape(32, 32, 3))\n",
        " pred_label = class_names[np.argmax(predictions[i])]\n",
        " true_label = class_names[test_labels[i][0]]\n",
        "\n",
        " color = 'green' if pred_label == true_label else 'red'\n",
        " plt.title(pred_label, color=color)\n",
        " plt.xlabel(f\"True: {true_label}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GTHOgBnRS6FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handwritten Digit Classification on MNIST using CNN (TensorFlow/Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "40\n",
        "\n",
        "model = models.Sequential([\n",
        " layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        " layers.MaxPooling2D((2, 2)),\n",
        " layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        " layers.MaxPooling2D((2, 2)),\n",
        " layers.Flatten(),\n",
        " layers.Dense(128, activation=\"relu\"),\n",
        " layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        " optimizer=\"adam\",\n",
        " loss=\"sparse_categorical_crossentropy\",\n",
        " metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
        "\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "img = x_test[4]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(\"Input Digit\")\n",
        "plt.show()\n",
        "pred = model.predict(np.expand_dims(img, axis=0), verbose=0)\n",
        "print(\"Predicted digit:\", pred.argmax())"
      ],
      "metadata": {
        "id": "DIo7ppmdTXFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Image Classification using Pre-Trained VGG16 (Transfer Learning on ImageNet)\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "model = VGG16(weights='imagenet')\n",
        "\n",
        "img_path = \"/content/OIP (1).jfif\"\n",
        "try:\n",
        " img = image.load_img(img_path, target_size=(224, 224))\n",
        "except FileNotFoundError:\n",
        " print(f\"Error: Image file not found at {img_path}\")\n",
        " exit()\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x, verbose=0)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.title(\"Input Image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])"
      ],
      "metadata": {
        "id": "l6bvIvjxUkkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}